{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "nnwqClrb8LUW"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/11/15 14:30:45 WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.master(\"local[*]\").appName(\"SparkMLLib_TFIDF\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Qjo4mnee8LUf",
    "outputId": "35e5f98e-0997-40f2-b9f2-25481e0aa4f3"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------+\n",
      "|label|            features|\n",
      "+-----+--------------------+\n",
      "|  0.0|(20,[6,8,13,16],[...|\n",
      "|  0.0|(20,[0,2,7,13,15,...|\n",
      "|  1.0|(20,[3,4,6,11,19]...|\n",
      "+-----+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import HashingTF, IDF, Tokenizer\n",
    "\n",
    "sentenceData = spark.createDataFrame([\n",
    "    (0.0, \"Hi I heard about Spark\"),\n",
    "    (0.0, \"I wish Java could use case classes\"),\n",
    "    (1.0, \"Logistic regression models are neat\")\n",
    "], [\"label\", \"sentence\"])\n",
    "\n",
    "tokenizer = Tokenizer(inputCol=\"sentence\", outputCol=\"words\")\n",
    "wordsData = tokenizer.transform(sentenceData)\n",
    "\n",
    "hashingTF = HashingTF(inputCol=\"words\", outputCol=\"rawFeatures\", numFeatures=20)\n",
    "featurizedData = hashingTF.transform(wordsData)\n",
    "# alternatively, CountVectorizer can also be used to get term frequency vectors\n",
    "\n",
    "idf = IDF(inputCol=\"rawFeatures\", outputCol=\"features\")\n",
    "idfModel = idf.fit(featurizedData)\n",
    "rescaledData = idfModel.transform(featurizedData)\n",
    "\n",
    "rescaledData.select(\"label\", \"features\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ncHmniua8LUq"
   },
   "source": [
    "### Dau tien tao DataFrame sentenceData chua cac label va cau. Tokenizer la 1 lop trong PySpark de phan tach chuoi van ban thanh cac tu rieng biet, sau do lay du lieu dau vao la cot \"sentence\" cua DataFrame sau do tien hanh phan tach va luu vao cot \"words\" wordsData\n",
    "### HashingTF la ky thuat anh xa cac tu thanh cac chi so trong khong gian vector co kich thuoc co dinh, thong qua mot ham bam. Lay du lieu tu cot \"words\" da tach o tren, voi so chieu cua vector la 20 thong qua tham so numFeatures. Sau do ap dung HashingTF len du lieu sau do luu vao featurizedData\n",
    "### IDF la ky thuat giup giam trong so cua cac tu xuat hien pho bien trong bo du lieu va tang trong so cua nhung tu it xuat hien de cai thien features, sau do ap dung len du lieu sau khi hashing. Sau do ap dung mo hinh len bo du lieu featurizedData. Cuoi cung la in ra gia tri cua cac cot \"label\" va \"features\" sau khi da ap dung IDF"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "SparkMLLib_TFIDF.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
