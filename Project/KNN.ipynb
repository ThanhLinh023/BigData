{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Emh3tdNqBJMw",
        "outputId": "6b5cecec-af42-4841-da16-746b4a638b9e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 81.70%\n",
            "Time Running: 833.38 seconds\n",
            "Confusion Matrix:\n",
            "      0     1\n",
            "0  1622   764\n",
            "1   553  4256\n",
            "\n",
            "Precision, Recall, F1-Score:\n",
            "   Precision    Recall  F1-Score\n",
            "0   0.745747  0.679799  0.711248\n",
            "1   0.847809  0.885007  0.866009\n"
          ]
        }
      ],
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from math import sqrt\n",
        "from collections import Counter\n",
        "import heapq\n",
        "import time\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# 1. Khởi tạo Spark Session\n",
        "spark = SparkSession.builder.appName(\"KNN_Spark_Optimized\").getOrCreate()\n",
        "sc = spark.sparkContext\n",
        "\n",
        "# 2. Đọc dữ liệu từ file CSV và chuyển thành RDD\n",
        "df = spark.read.csv('/content/drive/MyDrive/Final Year/BigData/Hotel_KNN.csv', header=True, inferSchema=True)\n",
        "data = df.rdd.map(lambda row: (row[:-1], row[-1]))  # (features, label)\n",
        "\n",
        "# 3. Chia dữ liệu thành train và test\n",
        "train_rdd, test_rdd = data.randomSplit([0.8, 0.2], seed=42)\n",
        "\n",
        "# 4. Chia nhỏ dữ liệu kiểm tra tương ứng với số partition của train_rdd\n",
        "num_partitions = 4\n",
        "train_rdd = train_rdd.repartition(num_partitions)\n",
        "test_data_partitions = test_rdd.zipWithIndex().map(lambda x: (x[1] % num_partitions, x[0]))\n",
        "test_data_groups = test_data_partitions.groupByKey().mapValues(list).collectAsMap()\n",
        "\n",
        "# 5. Broadcast dữ liệu kiểm tra được phân chia\n",
        "test_data_broadcast = sc.broadcast(test_data_groups)\n",
        "\n",
        "# 6. Hàm tính khoảng cách Euclidean\n",
        "def euclidean_distance(point1, point2):\n",
        "    return sqrt(sum((float(p1) - float(p2)) ** 2\n",
        "                    for p1, p2 in zip(point1, point2)\n",
        "                    if p1 is not None and p2 is not None))\n",
        "\n",
        "# 7. Hàm tính k-NN trên từng partition\n",
        "def knn_map_partition(partition_id, train_partition, k=5):\n",
        "    test_data = test_data_broadcast.value.get(partition_id, [])  # Lấy dữ liệu test tương ứng\n",
        "    results = []\n",
        "    train_list = list(train_partition)  # Chuyển dữ liệu huấn luyện thành list\n",
        "    for test_point in test_data:\n",
        "        test_features, test_label = test_point\n",
        "        k_neighbors = []\n",
        "        for train_features, train_label in train_list:\n",
        "            dist = euclidean_distance(test_features, train_features)\n",
        "            if len(k_neighbors) < k:\n",
        "                heapq.heappush(k_neighbors, (-dist, train_label))  # Min-Heap\n",
        "            else:\n",
        "                heapq.heappushpop(k_neighbors, (-dist, train_label))\n",
        "        k_neighbors = sorted([(-d, label) for d, label in k_neighbors])  # Sắp xếp k lân cận\n",
        "        results.append((test_label, k_neighbors))  # Lưu kết quả k-NN\n",
        "    return results\n",
        "\n",
        "# 8. Ánh xạ và gộp kết quả\n",
        "start_time = time.time()\n",
        "mapped_results = train_rdd.mapPartitionsWithIndex(\n",
        "    lambda idx, part: knn_map_partition(idx, part, k=5)\n",
        ")\n",
        "\n",
        "# 9. Bỏ phiếu đa số để dự đoán nhãn\n",
        "def reduce_phase(mapped_data):\n",
        "    test_label, k_neighbors = mapped_data\n",
        "    if not k_neighbors:  # Kiểm tra danh sách rỗng\n",
        "        return (test_label, None)\n",
        "    labels = [label for _, label in k_neighbors]\n",
        "    most_common_label = Counter(labels).most_common(1)[0][0]\n",
        "    return (test_label, most_common_label)\n",
        "\n",
        "predictions = mapped_results.map(reduce_phase).filter(lambda x: x[1] is not None)\n",
        "\n",
        "# 10. Tính Accuracy\n",
        "prediction_and_label = predictions.map(lambda x: (x[1], x[0]))  # (predicted_label, actual_label)\n",
        "correct_predictions = prediction_and_label.filter(lambda x: x[0] == x[1]).count()\n",
        "total_predictions = prediction_and_label.count()\n",
        "accuracy = correct_predictions / total_predictions if total_predictions > 0 else 0\n",
        "\n",
        "# 11. Tính ma trận nhầm lẫn\n",
        "confusion_matrix = prediction_and_label.map(lambda x: ((x[1], x[0]), 1)) \\\n",
        "    .reduceByKey(lambda a, b: a + b) \\\n",
        "    .collect()\n",
        "\n",
        "unique_labels = sorted(set(row[-1] for row in df.collect()))\n",
        "conf_matrix_df = pd.DataFrame(0, index=unique_labels, columns=unique_labels)\n",
        "\n",
        "for (actual, predicted), count in confusion_matrix:\n",
        "    conf_matrix_df.at[actual, predicted] = count\n",
        "\n",
        "# 12. Tính Precision, Recall và F1-score\n",
        "def calculate_metrics(conf_matrix):\n",
        "    metrics = {}\n",
        "    for label in conf_matrix.index:\n",
        "        tp = conf_matrix.at[label, label]  # True Positive\n",
        "        fp = conf_matrix[label].sum() - tp  # False Positive\n",
        "        fn = conf_matrix.loc[label].sum() - tp  # False Negative\n",
        "        precision = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
        "        recall = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
        "        f1 = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0\n",
        "        metrics[label] = {\"Precision\": precision, \"Recall\": recall, \"F1-Score\": f1}\n",
        "    return metrics\n",
        "\n",
        "metrics = calculate_metrics(conf_matrix_df)\n",
        "metrics_df = pd.DataFrame(metrics).T\n",
        "\n",
        "# 13. Hiển thị kết quả\n",
        "runtime = time.time() - start_time\n",
        "print(f\"Accuracy: {accuracy * 100:.2f}%\")\n",
        "print(f\"Time Running: {runtime:.2f} seconds\")\n",
        "print(\"Confusion Matrix:\")\n",
        "print(conf_matrix_df)\n",
        "print(\"\\nPrecision, Recall, F1-Score:\")\n",
        "print(metrics_df)\n",
        "\n",
        "# 14. Dừng Spark\n",
        "spark.stop()"
      ]
    }
  ]
}